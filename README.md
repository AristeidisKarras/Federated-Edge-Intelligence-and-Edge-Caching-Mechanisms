# Federated-Edge-Intelligence-and-Edge-Caching-Mechanisms
Federated Edge Intelligence and Edge Caching Mechanisms 

Link: https://www.mdpi.com/2078-2489/14/7/414

Abstract
Federated learning (FL) has emerged as a promising technique for preserving user privacy and ensuring data security in distributed machine learning contexts, particularly in edge intelligence and edge caching applications. Recognizing the prevalent challenges of imbalanced and noisy data impacting scalability and resilience, our study introduces two innovative algorithms crafted for FL within a peer-to-peer framework. These algorithms aim to enhance performance, especially in decentralized and resource-limited settings. Furthermore, we propose a client-balancing Dirichlet sampling algorithm with probabilistic guarantees to mitigate oversampling issues, optimizing data distribution among clients to achieve more accurate and reliable model training. Within the specifics of our study, we employed 10, 20, and 40 Raspberry Pi devices as clients in a practical FL scenario, simulating real-world conditions. The well-known FedAvg algorithm was implemented, enabling multi-epoch client training before weight integration. Additionally, we examined the influence of real-world dataset noise, culminating in a performance analysis that underscores how our novel methods and research significantly advance robust and efficient FL techniques, thereby enhancing the overall effectiveness of decentralized machine learning applications, including edge intelligence and edge caching.
Keywords: IoT; edge intelligence; edge ML; federated learning; edge caching mechanisms; large-scale IoT systems


Citation:
@Article{info14070414,
AUTHOR = {Karras, Aristeidis and Karras, Christos and Giotopoulos, Konstantinos C. and Tsolis, Dimitrios and Oikonomou, Konstantinos and Sioutas, Spyros},
TITLE = {Federated Edge Intelligence and Edge Caching Mechanisms},
JOURNAL = {Information},
VOLUME = {14},
YEAR = {2023},
NUMBER = {7},
ARTICLE-NUMBER = {414},
URL = {https://www.mdpi.com/2078-2489/14/7/414},
ISSN = {2078-2489},
ABSTRACT = {Federated learning (FL) has emerged as a promising technique for preserving user privacy and ensuring data security in distributed machine learning contexts, particularly in edge intelligence and edge caching applications. Recognizing the prevalent challenges of imbalanced and noisy data impacting scalability and resilience, our study introduces two innovative algorithms crafted for FL within a peer-to-peer framework. These algorithms aim to enhance performance, especially in decentralized and resource-limited settings. Furthermore, we propose a client-balancing Dirichlet sampling algorithm with probabilistic guarantees to mitigate oversampling issues, optimizing data distribution among clients to achieve more accurate and reliable model training. Within the specifics of our study, we employed 10, 20, and 40 Raspberry Pi devices as clients in a practical FL scenario, simulating real-world conditions. The well-known FedAvg algorithm was implemented, enabling multi-epoch client training before weight integration. Additionally, we examined the influence of real-world dataset noise, culminating in a performance analysis that underscores how our novel methods and research significantly advance robust and efficient FL techniques, thereby enhancing the overall effectiveness of decentralized machine learning applications, including edge intelligence and edge caching.},
DOI = {10.3390/info14070414}
}


